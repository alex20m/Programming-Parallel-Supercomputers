For Aalto & HY students
Setting up in LUMI
-------------------------

MAKING A CSC USER AND CONNECTING TO LUMI:
---------------------
1. Follow the instructions that you have received after completing the entry quiz
2. Test the SSH connection with `ssh <username>@lumi.csc.fi`
   - If you get a cute ASCII image of a wolf and a welcome message, then you are all set! Type `exit` to end the SSH session.

CONNECTING THROUGH THE LUMI WEB INTERFACE
---------------------
If you're rocking an exotic OS like Windows that makes it hard for you to connect via your terminal, 
you can always use the web interface for easier access:

https://www.lumi.csc.fi/public/

Choose to log in via HAKA with your University ID.
From there, you can choose a login node shell to access LUMI on the command line. 

FIRST STEPS
--------------

Navigate to the course project and create a personal working folder there:

mkdir -p /scratch/project_462001048/$USER​
cd /scratch/project_462001048/$USER

MODULE SYSTEM:
--------------

module list
module avail | grep partition
module spider partition
module load
module unload

Note! Do no use "module purge" on LUMI. 
If you need to reset your modules just log out/back in.

Settings for the course:

MPI & Hybrid
module load LUMI/24.03​
module load partition/C

HIP / GPU programming
module load LUMI/24.03​
module load partition/G​
module load rocm/6.0.3

C: MPI-compiler wrapper is cc
HIP: cc
      - MPI aware GPU communication might require the environment
      variable `export MPICH_GPU_SUPPORT_ENABLED=1` to be set

GitLab:
-------

version.aalto.fi: pps-example-codes
Recommended: set up SSH keys
git clone <copy repo link> repo-dir-name

Disks:
------

Check available areas in LUMI with the command:
lumi-workspaces

$HOME 
/projappl/<project-id>
/scratch/<project-id>

Return your submissions to /projappl/project_462001048/submissions/$USER

SLURM:
------
To run jobs:
sbatch job_script.sh

It's recommended to use debug/dev-g partitions, or small/small-g

squeue --me
scancel <jobid>

History of your jobs:
slurm h

scripts-dir: Example scripts for running MPI, MPI + openMP & CUDA + MPI jobs.


See also the LUMI documentation
--------

https://docs.lumi-supercomputer.eu/
